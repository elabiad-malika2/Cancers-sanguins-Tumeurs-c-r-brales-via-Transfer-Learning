{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba390d9",
   "metadata": {},
   "source": [
    "# üß† Projet Deep Learning ‚Äî Classification des Cellules Sanguines Canc√©reuses (PyTorch)\n",
    "\n",
    "## üìå 1. Contexte du Projet\n",
    "Vous √™tes un d√©veloppeur IA junior au sein d‚Äôun laboratoire biom√©dical sp√©cialis√© en imagerie m√©dicale.\n",
    "Objectif : Automatiser l‚Äôanalyse d‚Äôimages m√©dicales li√©es √† deux pathologies critiques :\n",
    "- D√©tection de **tumeurs c√©r√©brales** (object detection √† partir d‚ÄôIRM),\n",
    "- Classification de **cellules sanguines canc√©reuses** (leuc√©mies) √† partir de frottis sanguins.\n",
    "\n",
    "Cette premi√®re partie se concentre sur la **classification des cellules sanguines canc√©reuses avec PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31dee6",
   "metadata": {},
   "source": [
    "## üßæ 2. Importation des Biblioth√®ques N√©cessaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609de3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190bf28",
   "metadata": {},
   "source": [
    "## üìÇ 3. Chargement des Images & V√©rification des Extensions\n",
    "\n",
    "- Extensions autoris√©es : `.jpeg`, `.jpg`, `.png`, `.bmp`\n",
    "- Suppression des fichiers invalides\n",
    "- Gestion des erreurs via `try-except`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "\n",
    "# Parcourir tous les sous-dossiers (glioma, meningioma, etc.)\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            ext = os.path.splitext(file)[1].lower()  # extraire l‚Äôextension\n",
    "            \n",
    "            if ext not in valid_extensions:\n",
    "                os.remove(file_path)\n",
    "                print(f\" Fichier supprim√© : {file_path}\")\n",
    "            else:\n",
    "                print(f\" Fichier avec extension valide : {file_path}\")\n",
    "print(\" Suppression termin√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fd2f3",
   "metadata": {},
   "source": [
    "## üîé 4. Exploration des Classes du Dataset\n",
    "- Liste des dossiers (classes)\n",
    "- Nombre d‚Äôimages par classe (`countplot`)\n",
    "- Affichage d‚Äôun √©chantillon d‚Äôimages par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nbre d'image par classe :\n",
    "labels=[]\n",
    "images=[]\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path=os.path.join(data_dir,folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        count=0\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.lower().endswith(valid_extensions):\n",
    "                count+=1\n",
    "                images.append(os.path.join(folder_path, file))\n",
    "                labels.append(folder)\n",
    "        print(f\"Classe {folder} : {count} images\")\n",
    "print\n",
    "\n",
    "# affichage de nombre image par classe countplot \n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x=labels)\n",
    "plt.title(\"Nombre d'images par classe\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les images par classe \n",
    "\n",
    "classes = sorted(set(labels))\n",
    "print(classes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    # Trouver le premier chemin d'image appartenant √† cette classe\n",
    "    for img_path, label in zip(images, labels):\n",
    "        if label == cls:\n",
    "            img = Image.open(img_path)\n",
    "            plt.subplot(1, len(classes), i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(cls)\n",
    "            plt.axis(\"off\")\n",
    "            break  # on s'arr√™te apr√®s la premi√®re image trouv√©e\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6b8d8",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5. Division du Dataset en **Train / Validation / Test**\n",
    "- R√©partition : **70% / 15% / 15%**\n",
    "- V√©rification du nombre d‚Äôimages par dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "input_folder = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "output_folder = \"../Data/Processed\"\n",
    "\n",
    "splitfolders.ratio(\n",
    "    input_folder,\n",
    "    output=output_folder,\n",
    "    seed=42,\n",
    "    ratio=(0.7, 0.15, 0.15),\n",
    "    group_prefix=None,\n",
    "    move=False \n",
    ")\n",
    "\n",
    "print(\"Division des donn√©es termin√©e avec split-folders!\")\n",
    "\n",
    "train_path = os.path.join(output_folder, \"train\")\n",
    "val_path = os.path.join(output_folder, \"val\")\n",
    "test_path = os.path.join(output_folder, \"test\")\n",
    "\n",
<<<<<<< HEAD
    "base_path = \"..\\Data\\Processed\"\n",
    "train_path = os.path.join(base_path, \"Train\")\n",
    "val_path = os.path.join(base_path, \"Validation\")\n",
    "test_path = os.path.join(base_path, \"Test\")\n",
    "\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def copy_images_to_folder(image_paths, labels, destination_path):\n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        class_folder = os.path.join(destination_path, label)\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "        \n",
    "        img_name = os.path.basename(img_path)\n",
    "        dest_path = os.path.join(class_folder, img_name)\n",
    "        \n",
    "        try:\n",
    "            shutil.copy2(img_path, dest_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la copie de {img_path}: {e}\")\n",
    "\n",
    "copy_images_to_folder(x_train, y_train, train_path)\n",
    "\n",
    "copy_images_to_folder(x_val, y_val, val_path)\n",
    "\n",
    "copy_images_to_folder(x_test, y_test, test_path)\n",
    "\n",
    "print(\"Repartition par classe :\")\n",
=======
    "print(\"\\nR√©partition par classe :\")\n",
>>>>>>> issam
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5b329",
   "metadata": {},
   "source": [
    "## üîÑ 6. Data Augmentation (Seulement sur Train)\n",
    "- Transformations : `blur`, `noise`, `flip`\n",
    "- Objectif : √©quilibrer les classes et augmenter la robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db60e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_path = \"../Data/Processed/train\"\n",
    "\n",
    "def add_noise(img, noise_level=20):\n",
    "    arr = np.array(img)\n",
    "    noise = np.random.randint(-noise_level, noise_level, arr.shape, dtype='int16')\n",
    "    noisy_arr = np.clip(arr.astype('int16') + noise, 0, 255).astype('uint8')\n",
    "    return Image.fromarray(noisy_arr)\n",
    "\n",
    "def augment_image(img_path, save_dir, suffix):\n",
    "    \"\"\"Apply combined augmentation (blur + noise + flip) and save with suffix\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    ext = os.path.splitext(img_path)[1]\n",
    "    \n",
    "    blurred = img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    noisy = add_noise(blurred)\n",
    "    flipped = noisy.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    \n",
    "    new_name = f\"{base_name}_aug{suffix}{ext}\"\n",
    "    flipped.save(os.path.join(save_dir, new_name))\n",
    "    \n",
    "    return 1\n",
    "\n",
    "TARGET_COUNT = 688\n",
    "\n",
    "print(\"Starting augmentation to balance classes to 688 images each...\")\n",
    "\n",
    "for class_name in os.listdir(train_path):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    \n",
    "    all_images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    original_images = [f for f in all_images if '_aug' not in f and '_dup' not in f and '_flip' not in f]\n",
    "    \n",
    "    current_count = len(all_images)\n",
    "    print(f\"\\n Class '{class_name}':\")\n",
    "    print(f\"   Current: {current_count} images\")\n",
    "    print(f\"   Target: {TARGET_COUNT} images\")\n",
    "    \n",
    "    if current_count >= TARGET_COUNT:\n",
    "        print(f\"    Already at or above target\")\n",
    "        continue\n",
    "    \n",
    "    needed = TARGET_COUNT - current_count\n",
    "    print(f\"  Need to add: {needed} augmented images\")\n",
    "    \n",
    "    aug_counter = 0\n",
    "    while current_count < TARGET_COUNT:\n",
    "        img_file = original_images[aug_counter % len(original_images)]\n",
    "        img_path = os.path.join(class_dir, img_file)\n",
    "        \n",
    "        augment_image(img_path, class_dir, aug_counter)\n",
    "        \n",
    "        aug_counter += 1\n",
    "        current_count += 1\n",
    "    \n",
    "    print(f\"    Created {aug_counter} augmented images\")\n",
    "\n",
    "print(\"\\n Augmentation completed!\")\n",
    "\n",
    "print(\"\\n Final Distribution:\")\n",
    "for class_name in sorted(os.listdir(train_path)):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        count = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "        print(f\"   - {class_name}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre √† jour les chemins (split-folders cr√©e train/val/test en minuscules)\n",
    "train_path = \"../Data/Processed/train\"\n",
    "val_path = \"../Data/Processed/val\"\n",
    "test_path = \"../Data/Processed/test\"\n",
    "\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a628",
   "metadata": {},
   "source": [
    "## üß™ 7. Pr√©paration des Donn√©es avec `ImageFolder` & `Transforms`\n",
    "- Resize\n",
    "- ToTensor\n",
    "- Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3619b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "\n",
    "# Chemins de train/test/validation (split-folders utilise des noms en minuscules)\n",
    "train_dir = \"../Data/Processed/train\"\n",
    "test_dir = \"../Data/Processed/test\"\n",
    "validation_dir = \"../Data/Processed/val\"\n",
    "\n",
    "# Definition de transformer :\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], # Normaliser (R, G, B)\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Charger les datasets avec ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "validation_dataset   = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# V√©rification\n",
    "print(\"Nombre d'images dans le train :\", len(train_dataset))\n",
    "print(\"Nombre d'images dans la validation :\", len(validation_dataset))\n",
    "print(\"Nombre d'images dans le test :\", len(test_dataset))\n",
    "print(\"Classes :\", train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5faa3",
   "metadata": {},
   "source": [
    "## üöö 8. Cr√©ation des DataLoaders\n",
    "- Batch loading\n",
    "- Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0951c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Validation dataset: {len(validation_dataset)} images\")\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860eb724",
   "metadata": {},
   "source": [
    "## üß† 9. Chargement du Mod√®le Pr√©-entra√Æn√© **GoogLeNet**\n",
    "- Remplacement de la partie **Fully Connected (FC)** par un `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db743dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Nombre de classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Charger le mod√®le pr√©-entra√Æn√©\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Geler les couches convolutives\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Remplacer la couche fully connected\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Pas besoin de device, on reste sur CPU\n",
    "print(\"Mod√®le pr√™t sur CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef946b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 10. Configuration de l‚ÄôEntra√Ænement\n",
    "- Learning Rate\n",
    "- Loss Function\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb35a6",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 11. Boucle d‚ÄôEntra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Stocker les valeurs\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfaa28",
   "metadata": {},
   "source": [
    "\n",
    "## üìä 12. √âvaluation & Test du Mod√®le\n",
    "- Accuracy / Loss\n",
    "- Matrice de confusion (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a79ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_preds.extend(predicted.tolist())\n",
    "\n",
    "# Calculer l'accuracy\n",
    "test_acc = sum([a==b for a,b in zip(all_labels, all_preds)]) / len(all_labels)\n",
    "print(f\"Accuracy sur le test set : {test_acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Matrice de confusion :\\n\", cm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"V√©ritables classes\")\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs+1)\n",
    "\n",
    "# Loss\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"√âvolution du Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"√âvolution de l'Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e79b",
   "metadata": {},
   "source": [
    "## üíæ 13. Sauvegarde du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../models/googlenet_complete.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e240e",
   "metadata": {},
   "source": [
    "## ‚úÖ 14. Conclusion & Observations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
