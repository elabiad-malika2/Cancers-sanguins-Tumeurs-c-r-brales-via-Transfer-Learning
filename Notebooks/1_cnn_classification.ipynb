{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba390d9",
   "metadata": {},
   "source": [
    "# 🧠 Projet Deep Learning — Classification des Cellules Sanguines Cancéreuses (PyTorch)\n",
    "\n",
    "## 📌 1. Contexte du Projet\n",
    "Vous êtes un développeur IA junior au sein d’un laboratoire biomédical spécialisé en imagerie médicale.\n",
    "Objectif : Automatiser l’analyse d’images médicales liées à deux pathologies critiques :\n",
    "- Détection de **tumeurs cérébrales** (object detection à partir d’IRM),\n",
    "- Classification de **cellules sanguines cancéreuses** (leucémies) à partir de frottis sanguins.\n",
    "\n",
    "Cette première partie se concentre sur la **classification des cellules sanguines cancéreuses avec PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31dee6",
   "metadata": {},
   "source": [
    "## 🧾 2. Importation des Bibliothèques Nécessaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609de3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190bf28",
   "metadata": {},
   "source": [
    "## 📂 3. Chargement des Images & Vérification des Extensions\n",
    "\n",
    "- Extensions autorisées : `.jpeg`, `.jpg`, `.png`, `.bmp`\n",
    "- Suppression des fichiers invalides\n",
    "- Gestion des erreurs via `try-except`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "\n",
    "# Parcourir tous les sous-dossiers (glioma, meningioma, etc.)\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            ext = os.path.splitext(file)[1].lower()  # extraire l’extension\n",
    "            \n",
    "            if ext not in valid_extensions:\n",
    "                os.remove(file_path)\n",
    "                print(f\" Fichier supprimé : {file_path}\")\n",
    "            else:\n",
    "                print(f\" Fichier avec extension valide : {file_path}\")\n",
    "print(\" Suppression terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fd2f3",
   "metadata": {},
   "source": [
    "## 🔎 4. Exploration des Classes du Dataset\n",
    "- Liste des dossiers (classes)\n",
    "- Nombre d’images par classe (`countplot`)\n",
    "- Affichage d’un échantillon d’images par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nbre d'image par classe :\n",
    "labels=[]\n",
    "images=[]\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path=os.path.join(data_dir,folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        count=0\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.lower().endswith(valid_extensions):\n",
    "                count+=1\n",
    "                images.append(os.path.join(folder_path, file))\n",
    "                labels.append(folder)\n",
    "        print(f\"Classe {folder} : {count} images\")\n",
    "print\n",
    "\n",
    "# affichage de nombre image par classe countplot \n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x=labels)\n",
    "plt.title(\"Nombre d'images par classe\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les images par classe \n",
    "\n",
    "classes = sorted(set(labels))\n",
    "print(classes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    # Trouver le premier chemin d'image appartenant à cette classe\n",
    "    for img_path, label in zip(images, labels):\n",
    "        if label == cls:\n",
    "            img = Image.open(img_path)\n",
    "            plt.subplot(1, len(classes), i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(cls)\n",
    "            plt.axis(\"off\")\n",
    "            break  # on s'arrête après la première image trouvée\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6b8d8",
   "metadata": {},
   "source": [
    "## ✂️ 5. Division du Dataset en **Train / Validation / Test**\n",
    "- Répartition : **70% / 15% / 15%**\n",
    "- Vérification du nombre d’images par dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "input_folder = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "output_folder = \"../Data/Processed\"\n",
    "\n",
    "splitfolders.ratio(\n",
    "    input_folder,\n",
    "    output=output_folder,\n",
    "    seed=42,\n",
    "    ratio=(0.7, 0.15, 0.15),\n",
    "    group_prefix=None,\n",
    "    move=False \n",
    ")\n",
    "\n",
    "print(\"Division des données terminée avec split-folders!\")\n",
    "\n",
    "train_path = os.path.join(output_folder, \"train\")\n",
    "val_path = os.path.join(output_folder, \"val\")\n",
    "test_path = os.path.join(output_folder, \"test\")\n",
    "\n",
    "print(\"\\nRépartition par classe :\")\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5b329",
   "metadata": {},
   "source": [
    "## 🔄 6. Data Augmentation (Seulement sur Train)\n",
    "- Transformations : `blur`, `noise`, `flip`\n",
    "- Objectif : équilibrer les classes et augmenter la robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db60e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_path = \"../Data/Processed/train\"\n",
    "\n",
    "def add_noise(img, noise_level=20):\n",
    "    arr = np.array(img)\n",
    "    noise = np.random.randint(-noise_level, noise_level, arr.shape, dtype='int16')\n",
    "    noisy_arr = np.clip(arr.astype('int16') + noise, 0, 255).astype('uint8')\n",
    "    return Image.fromarray(noisy_arr)\n",
    "\n",
    "def augment_image(img_path, save_dir, suffix):\n",
    "    \"\"\"Apply combined augmentation (blur + noise + flip) and save with suffix\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    ext = os.path.splitext(img_path)[1]\n",
    "    \n",
    "    blurred = img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    noisy = add_noise(blurred)\n",
    "    flipped = noisy.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    \n",
    "    new_name = f\"{base_name}_aug{suffix}{ext}\"\n",
    "    flipped.save(os.path.join(save_dir, new_name))\n",
    "    \n",
    "    return 1\n",
    "\n",
    "TARGET_COUNT = 688\n",
    "\n",
    "print(\"Starting augmentation to balance classes to 688 images each...\")\n",
    "\n",
    "for class_name in os.listdir(train_path):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    \n",
    "    all_images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    original_images = [f for f in all_images if '_aug' not in f and '_dup' not in f and '_flip' not in f]\n",
    "    \n",
    "    current_count = len(all_images)\n",
    "    print(f\"\\n Class '{class_name}':\")\n",
    "    print(f\"   Current: {current_count} images\")\n",
    "    print(f\"   Target: {TARGET_COUNT} images\")\n",
    "    \n",
    "    if current_count >= TARGET_COUNT:\n",
    "        print(f\"    Already at or above target\")\n",
    "        continue\n",
    "    \n",
    "    needed = TARGET_COUNT - current_count\n",
    "    print(f\"  Need to add: {needed} augmented images\")\n",
    "    \n",
    "    aug_counter = 0\n",
    "    while current_count < TARGET_COUNT:\n",
    "        img_file = original_images[aug_counter % len(original_images)]\n",
    "        img_path = os.path.join(class_dir, img_file)\n",
    "        \n",
    "        augment_image(img_path, class_dir, aug_counter)\n",
    "        \n",
    "        aug_counter += 1\n",
    "        current_count += 1\n",
    "    \n",
    "    print(f\"    Created {aug_counter} augmented images\")\n",
    "\n",
    "print(\"\\n Augmentation completed!\")\n",
    "\n",
    "print(\"\\n Final Distribution:\")\n",
    "for class_name in sorted(os.listdir(train_path)):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        count = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "        print(f\"   - {class_name}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre à jour les chemins (split-folders crée train/val/test en minuscules)\n",
    "train_path = \"../Data/Processed/train\"\n",
    "val_path = \"../Data/Processed/val\"\n",
    "test_path = \"../Data/Processed/test\"\n",
    "\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a628",
   "metadata": {},
   "source": [
    "## 🧪 7. Préparation des Données avec `ImageFolder` & `Transforms`\n",
    "- Resize\n",
    "- ToTensor\n",
    "- Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3619b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "\n",
    "# Chemins de train/test/validation (split-folders utilise des noms en minuscules)\n",
    "train_dir = \"../Data/Processed/train\"\n",
    "test_dir = \"../Data/Processed/test\"\n",
    "validation_dir = \"../Data/Processed/val\"\n",
    "\n",
    "# Definition de transformer :\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], # Normaliser (R, G, B)\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Charger les datasets avec ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "validation_dataset   = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Vérification\n",
    "print(\"Nombre d'images dans le train :\", len(train_dataset))\n",
    "print(\"Nombre d'images dans la validation :\", len(validation_dataset))\n",
    "print(\"Nombre d'images dans le test :\", len(test_dataset))\n",
    "print(\"Classes :\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5faa3",
   "metadata": {},
   "source": [
    "## 🚚 8. Création des DataLoaders\n",
    "- Batch loading\n",
    "- Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0951c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Validation dataset: {len(validation_dataset)} images\")\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860eb724",
   "metadata": {},
   "source": [
    "## 🧠 9. Chargement du Modèle Pré-entraîné **GoogLeNet**\n",
    "- Remplacement de la partie **Fully Connected (FC)** par un `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db743dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Nombre de classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Charger le modèle pré-entraîné\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Geler les couches convolutives\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Remplacer la couche fully connected\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Pas besoin de device, on reste sur CPU\n",
    "print(\"Modèle prêt sur CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef946b",
   "metadata": {},
   "source": [
    "## ⚙️ 10. Configuration de l’Entraînement\n",
    "- Learning Rate\n",
    "- Loss Function\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb35a6",
   "metadata": {},
   "source": [
    "## 🏋️ 11. Boucle d’Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Stocker les valeurs\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfaa28",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 12. Évaluation & Test du Modèle\n",
    "- Accuracy / Loss\n",
    "- Matrice de confusion (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a79ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_preds.extend(predicted.tolist())\n",
    "\n",
    "# Calculer l'accuracy\n",
    "test_acc = sum([a==b for a,b in zip(all_labels, all_preds)]) / len(all_labels)\n",
    "print(f\"Accuracy sur le test set : {test_acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Matrice de confusion :\\n\", cm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "plt.xlabel(\"Prédictions\")\n",
    "plt.ylabel(\"Véritables classes\")\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs+1)\n",
    "\n",
    "# Loss\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Évolution du Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Évolution de l'Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e79b",
   "metadata": {},
   "source": [
    "## 💾 13. Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../models/googlenet_complete.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e240e",
   "metadata": {},
   "source": [
    "## ✅ 14. Conclusion & Observations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
