{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba390d9",
   "metadata": {},
   "source": [
    "# üß† Projet Deep Learning ‚Äî Classification des Cellules Sanguines Canc√©reuses (PyTorch)\n",
    "\n",
    "## üìå 1. Contexte du Projet\n",
    "Vous √™tes un d√©veloppeur IA junior au sein d‚Äôun laboratoire biom√©dical sp√©cialis√© en imagerie m√©dicale.\n",
    "Objectif : Automatiser l‚Äôanalyse d‚Äôimages m√©dicales li√©es √† deux pathologies critiques :\n",
    "- D√©tection de **tumeurs c√©r√©brales** (object detection √† partir d‚ÄôIRM),\n",
    "- Classification de **cellules sanguines canc√©reuses** (leuc√©mies) √† partir de frottis sanguins.\n",
    "\n",
    "Cette premi√®re partie se concentre sur la **classification des cellules sanguines canc√©reuses avec PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31dee6",
   "metadata": {},
   "source": [
    "## üßæ 2. Importation des Biblioth√®ques N√©cessaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609de3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190bf28",
   "metadata": {},
   "source": [
    "## üìÇ 3. Chargement des Images & V√©rification des Extensions\n",
    "\n",
    "- Extensions autoris√©es : `.jpeg`, `.jpg`, `.png`, `.bmp`\n",
    "- Suppression des fichiers invalides\n",
    "- Gestion des erreurs via `try-except`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "\n",
    "# Parcourir tous les sous-dossiers (glioma, meningioma, etc.)\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            ext = os.path.splitext(file)[1].lower()  # extraire l‚Äôextension\n",
    "            \n",
    "            if ext not in valid_extensions:\n",
    "                os.remove(file_path)\n",
    "                print(f\" Fichier supprim√© : {file_path}\")\n",
    "            else:\n",
    "                print(f\" Fichier avec extension valide : {file_path}\")\n",
    "print(\" Suppression termin√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fd2f3",
   "metadata": {},
   "source": [
    "## üîé 4. Exploration des Classes du Dataset\n",
    "- Liste des dossiers (classes)\n",
    "- Nombre d‚Äôimages par classe (`countplot`)\n",
    "- Affichage d‚Äôun √©chantillon d‚Äôimages par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nbre d'image par classe :\n",
    "labels=[]\n",
    "images=[]\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path=os.path.join(data_dir,folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        count=0\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.lower().endswith(valid_extensions):\n",
    "                count+=1\n",
    "                images.append(os.path.join(folder_path, file))\n",
    "                labels.append(folder)\n",
    "        print(f\"Classe {folder} : {count} images\")\n",
    "print\n",
    "\n",
    "# affichage de nombre image par classe countplot \n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x=labels)\n",
    "plt.title(\"Nombre d'images par classe\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les images par classe \n",
    "\n",
    "classes = sorted(set(labels))\n",
    "print(classes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    # Trouver le premier chemin d'image appartenant √† cette classe\n",
    "    for img_path, label in zip(images, labels):\n",
    "        if label == cls:\n",
    "            img = Image.open(img_path)\n",
    "            plt.subplot(1, len(classes), i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(cls)\n",
    "            plt.axis(\"off\")\n",
    "            break  # on s'arr√™te apr√®s la premi√®re image trouv√©e\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6b8d8",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5. Division du Dataset en **Train / Validation / Test**\n",
    "- R√©partition : **70% / 15% / 15%**\n",
    "- V√©rification du nombre d‚Äôimages par dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    images, labels,\n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_test, y_test,\n",
    "    test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "base_path = \"..\\Data\\Processed\"\n",
    "base_path = \"..\\Data\\Processed\"\n",
    "train_path = os.path.join(base_path, \"Train\")\n",
    "val_path = os.path.join(base_path, \"Validation\")\n",
    "test_path = os.path.join(base_path, \"Test\")\n",
    "\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def copy_images_to_folder(image_paths, labels, destination_path):\n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        class_folder = os.path.join(destination_path, label)\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "        \n",
    "        img_name = os.path.basename(img_path)\n",
    "        dest_path = os.path.join(class_folder, img_name)\n",
    "        \n",
    "        try:\n",
    "            shutil.copy2(img_path, dest_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la copie de {img_path}: {e}\")\n",
    "\n",
    "copy_images_to_folder(x_train, y_train, train_path)\n",
    "\n",
    "copy_images_to_folder(x_val, y_val, val_path)\n",
    "\n",
    "copy_images_to_folder(x_test, y_test, test_path)\n",
    "\n",
    "print(\"Repartition par classe :\")\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5b329",
   "metadata": {},
   "source": [
    "## üîÑ 6. Data Augmentation (Seulement sur Train)\n",
    "- Transformations : `blur`, `noise`, `flip`\n",
    "- Objectif : √©quilibrer les classes et augmenter la robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db60e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def add_noise(img, noise_level=20):\n",
    "    arr = np.array(img)\n",
    "    noise = np.random.randint(-noise_level, noise_level, arr.shape, dtype='int16')\n",
    "    noisy_arr = np.clip(arr.astype('int16') + noise, 0, 255).astype('uint8')\n",
    "    return Image.fromarray(noisy_arr)\n",
    "\n",
    "def augment_image(img_path, save_dir, augmentations):\n",
    "    img = Image.open(img_path)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    ext = os.path.splitext(img_path)[1]\n",
    "    aug_count = 0\n",
    "\n",
    "    if 'blur' in augmentations:\n",
    "        blurred = img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "        blurred.save(os.path.join(save_dir, f\"{base_name}_blur{ext}\"))\n",
    "        aug_count += 1\n",
    "\n",
    "    if 'noise' in augmentations:\n",
    "        noisy = add_noise(img)\n",
    "        noisy.save(os.path.join(save_dir, f\"{base_name}_noise{ext}\"))\n",
    "        aug_count += 1\n",
    "\n",
    "    if 'flip' in augmentations:\n",
    "        flipped = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flipped.save(os.path.join(save_dir, f\"{base_name}_flip{ext}\"))\n",
    "        aug_count += 1\n",
    "\n",
    "    return aug_count\n",
    "\n",
    "for class_name in os.listdir(train_path):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    for img_file in images:\n",
    "        img_path = os.path.join(class_dir, img_file)\n",
    "        augment_image(img_path, class_dir, augmentations=['blur', 'noise', 'flip'])\n",
    "\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def balance_classes_oversample(dataset_path):\n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    \n",
    "    class_counts = {}\n",
    "    class_images = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        class_counts[class_name] = len(images)\n",
    "        class_images[class_name] = images\n",
    "    \n",
    "    max_count = max(class_counts.values())\n",
    " \n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        images = class_images[class_name]\n",
    "        current_count = len(images)\n",
    "        \n",
    "        if current_count < max_count:\n",
    "            random.seed(42) \n",
    "            num_to_add = max_count - current_count\n",
    "            \n",
    "            images_to_duplicate = random.choices(images, k=num_to_add)\n",
    "            \n",
    "            for idx, img_file in enumerate(images_to_duplicate):\n",
    "                src_path = os.path.join(class_dir, img_file)\n",
    "                \n",
    "                base_name, ext = os.path.splitext(img_file)\n",
    "                new_name = f\"{base_name}_dup{idx}{ext}\"\n",
    "                dst_path = os.path.join(class_dir, new_name)\n",
    "                \n",
    "                shutil.copy2(src_path, dst_path)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "balance_classes_oversample(train_path)\n",
    "balance_classes_oversample(val_path)\n",
    "balance_classes_oversample(test_path)\n",
    "\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a628",
   "metadata": {},
   "source": [
    "## üß™ 7. Pr√©paration des Donn√©es avec `ImageFolder` & `Transforms`\n",
    "- Resize\n",
    "- ToTensor\n",
    "- Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3619b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "\n",
    "# Chemins de train/test/validation\n",
    "train_dir = \"../Data/Processed/Train\"\n",
    "test_dir = \"../Data/Processed/Test\"\n",
    "validation_dir = \"../Data/Processed/Validation\"\n",
    "\n",
    "# Definition de transformer :\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], # Normaliser (R, G, B)\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Charger les datasets avec ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "validation_dataset   = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# V√©rification\n",
    "print(\"Nombre d'images dans le train :\", len(train_dataset))\n",
    "print(\"Nombre d'images dans la validation :\", len(validation_dataset))\n",
    "print(\"Nombre d'images dans le test :\", len(test_dataset))\n",
    "print(\"Classes :\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5faa3",
   "metadata": {},
   "source": [
    "## üöö 8. Cr√©ation des DataLoaders\n",
    "- Batch loading\n",
    "- Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0951c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(root=val_path, transform=transform_val_test)\n",
    "test_dataset = datasets.ImageFolder(root=test_path, transform=transform_val_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} images\")\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860eb724",
   "metadata": {},
   "source": [
    "## üß† 9. Chargement du Mod√®le Pr√©-entra√Æn√© **GoogLeNet**\n",
    "- Remplacement de la partie **Fully Connected (FC)** par un `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db743dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ef946b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 10. Configuration de l‚ÄôEntra√Ænement\n",
    "- Learning Rate\n",
    "- Loss Function\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47bcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ecb35a6",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 11. Boucle d‚ÄôEntra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3aff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42cfaa28",
   "metadata": {},
   "source": [
    "\n",
    "## üìä 12. √âvaluation & Test du Mod√®le\n",
    "- Accuracy / Loss\n",
    "- Matrice de confusion (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a79ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c77e79b",
   "metadata": {},
   "source": [
    "## üíæ 13. Sauvegarde du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23228f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20e240e",
   "metadata": {},
   "source": [
    "## ‚úÖ 14. Conclusion & Observations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
