{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba390d9",
   "metadata": {},
   "source": [
    "# ðŸ§  Projet Deep Learning â€” Classification des Cellules Sanguines CancÃ©reuses (PyTorch)\n",
    "\n",
    "## ðŸ“Œ 1. Contexte du Projet\n",
    "Vous Ãªtes un dÃ©veloppeur IA junior au sein dâ€™un laboratoire biomÃ©dical spÃ©cialisÃ© en imagerie mÃ©dicale.\n",
    "Objectif : Automatiser lâ€™analyse dâ€™images mÃ©dicales liÃ©es Ã  deux pathologies critiques :\n",
    "- DÃ©tection de **tumeurs cÃ©rÃ©brales** (object detection Ã  partir dâ€™IRM),\n",
    "- Classification de **cellules sanguines cancÃ©reuses** (leucÃ©mies) Ã  partir de frottis sanguins.\n",
    "\n",
    "Cette premiÃ¨re partie se concentre sur la **classification des cellules sanguines cancÃ©reuses avec PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31dee6",
   "metadata": {},
   "source": [
    "## ðŸ§¾ 2. Importation des BibliothÃ¨ques NÃ©cessaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609de3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190bf28",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 3. Chargement des Images & VÃ©rification des Extensions\n",
    "\n",
    "- Extensions autorisÃ©es : `.jpeg`, `.jpg`, `.png`, `.bmp`\n",
    "- Suppression des fichiers invalides\n",
    "- Gestion des erreurs via `try-except`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "\n",
    "# Parcourir tous les sous-dossiers (glioma, meningioma, etc.)\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            ext = os.path.splitext(file)[1].lower()  # extraire lâ€™extension\n",
    "            \n",
    "            if ext not in valid_extensions:\n",
    "                os.remove(file_path)\n",
    "                print(f\" Fichier supprimÃ© : {file_path}\")\n",
    "            else:\n",
    "                print(f\" Fichier avec extension valide : {file_path}\")\n",
    "print(\" Suppression terminÃ©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fd2f3",
   "metadata": {},
   "source": [
    "## ðŸ”Ž 4. Exploration des Classes du Dataset\n",
    "- Liste des dossiers (classes)\n",
    "- Nombre dâ€™images par classe (`countplot`)\n",
    "- Affichage dâ€™un Ã©chantillon dâ€™images par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nbre d'image par classe :\n",
    "labels=[]\n",
    "images=[]\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path=os.path.join(data_dir,folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        count=0\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.lower().endswith(valid_extensions):\n",
    "                count+=1\n",
    "                images.append(os.path.join(folder_path, file))\n",
    "                labels.append(folder)\n",
    "        print(f\"Classe {folder} : {count} images\")\n",
    "print\n",
    "\n",
    "# affichage de nombre image par classe countplot \n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x=labels)\n",
    "plt.title(\"Nombre d'images par classe\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les images par classe \n",
    "\n",
    "classes = sorted(set(labels))\n",
    "print(classes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    # Trouver le premier chemin d'image appartenant Ã  cette classe\n",
    "    for img_path, label in zip(images, labels):\n",
    "        if label == cls:\n",
    "            img = Image.open(img_path)\n",
    "            plt.subplot(1, len(classes), i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(cls)\n",
    "            plt.axis(\"off\")\n",
    "            break  # on s'arrÃªte aprÃ¨s la premiÃ¨re image trouvÃ©e\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6b8d8",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ 5. Division du Dataset en **Train / Validation / Test**\n",
    "- RÃ©partition : **70% / 15% / 15%**\n",
    "- VÃ©rification du nombre dâ€™images par dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "input_folder = \"../Data/Raw/Blood cell Cancer [ALL]\"\n",
    "output_folder = \"../Data/Processed\"\n",
    "\n",
    "splitfolders.ratio(\n",
    "    input_folder,\n",
    "    output=output_folder,\n",
    "    seed=42,\n",
    "    ratio=(0.7, 0.15, 0.15),\n",
    "    group_prefix=None,\n",
    "    move=False \n",
    ")\n",
    "\n",
    "print(\"Division des donnÃ©es terminÃ©e avec split-folders!\")\n",
    "\n",
    "train_path = os.path.join(output_folder, \"train\")\n",
    "val_path = os.path.join(output_folder, \"val\")\n",
    "test_path = os.path.join(output_folder, \"test\")\n",
    "\n",
    "print(\"\\nRÃ©partition par classe :\")\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5b329",
   "metadata": {},
   "source": [
    "## ðŸ”„ 6. Data Augmentation (Seulement sur Train)\n",
    "- Transformations : `blur`, `noise`, `flip`\n",
    "- Objectif : Ã©quilibrer les classes et augmenter la robustesse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db60e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_path = \"../Data/Processed/train\"\n",
    "\n",
    "def add_noise(img, noise_level=20):\n",
    "    arr = np.array(img)\n",
    "    noise = np.random.randint(-noise_level, noise_level, arr.shape, dtype='int16')\n",
    "    noisy_arr = np.clip(arr.astype('int16') + noise, 0, 255).astype('uint8')\n",
    "    return Image.fromarray(noisy_arr)\n",
    "\n",
    "def augment_image(img_path, save_dir, suffix):\n",
    "    \"\"\"Apply combined augmentation (blur + noise + flip) and save with suffix\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    ext = os.path.splitext(img_path)[1]\n",
    "    \n",
    "    blurred = img.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    noisy = add_noise(blurred)\n",
    "    flipped = noisy.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    \n",
    "    new_name = f\"{base_name}_aug{suffix}{ext}\"\n",
    "    flipped.save(os.path.join(save_dir, new_name))\n",
    "    \n",
    "    return 1\n",
    "\n",
    "TARGET_COUNT = 688\n",
    "\n",
    "print(\"Starting augmentation to balance classes to 688 images each...\")\n",
    "\n",
    "for class_name in os.listdir(train_path):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    \n",
    "    all_images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    original_images = [f for f in all_images if '_aug' not in f and '_dup' not in f and '_flip' not in f]\n",
    "    \n",
    "    current_count = len(all_images)\n",
    "    print(f\"\\n Class '{class_name}':\")\n",
    "    print(f\"   Current: {current_count} images\")\n",
    "    print(f\"   Target: {TARGET_COUNT} images\")\n",
    "    \n",
    "    if current_count >= TARGET_COUNT:\n",
    "        print(f\"    Already at or above target\")\n",
    "        continue\n",
    "    \n",
    "    needed = TARGET_COUNT - current_count\n",
    "    print(f\"  Need to add: {needed} augmented images\")\n",
    "    \n",
    "    aug_counter = 0\n",
    "    while current_count < TARGET_COUNT:\n",
    "        img_file = original_images[aug_counter % len(original_images)]\n",
    "        img_path = os.path.join(class_dir, img_file)\n",
    "        \n",
    "        augment_image(img_path, class_dir, aug_counter)\n",
    "        \n",
    "        aug_counter += 1\n",
    "        current_count += 1\n",
    "    \n",
    "    print(f\"    Created {aug_counter} augmented images\")\n",
    "\n",
    "print(\"\\n Augmentation completed!\")\n",
    "\n",
    "print(\"\\n Final Distribution:\")\n",
    "for class_name in sorted(os.listdir(train_path)):\n",
    "    class_dir = os.path.join(train_path, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        count = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "        print(f\"   - {class_name}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre Ã  jour les chemins (split-folders crÃ©e train/val/test en minuscules)\n",
    "train_path = \"../Data/Processed/train\"\n",
    "val_path = \"../Data/Processed/val\"\n",
    "test_path = \"../Data/Processed/test\"\n",
    "\n",
    "for dataset_name, dataset_path in [(\"Train\", train_path), (\"Validation\", val_path), (\"Test\", test_path)]:\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    if os.path.exists(dataset_path):\n",
    "        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "        for class_name in sorted(classes):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            print(f\"   - {class_name}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a628",
   "metadata": {},
   "source": [
    "## ðŸ§ª 7. PrÃ©paration des DonnÃ©es avec `ImageFolder` & `Transforms`\n",
    "- Resize\n",
    "- ToTensor\n",
    "- Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3619b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "\n",
    "# Chemins de train/test/validation (split-folders utilise des noms en minuscules)\n",
    "train_dir = \"../Data/Processed/train\"\n",
    "test_dir = \"../Data/Processed/test\"\n",
    "validation_dir = \"../Data/Processed/val\"\n",
    "\n",
    "# Definition de transformer :\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], # Normaliser (R, G, B)\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Charger les datasets avec ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "validation_dataset   = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# VÃ©rification\n",
    "print(\"Nombre d'images dans le train :\", len(train_dataset))\n",
    "print(\"Nombre d'images dans la validation :\", len(validation_dataset))\n",
    "print(\"Nombre d'images dans le test :\", len(test_dataset))\n",
    "print(\"Classes :\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5faa3",
   "metadata": {},
   "source": [
    "## ðŸšš 8. CrÃ©ation des DataLoaders\n",
    "- Batch loading\n",
    "- Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0951c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Validation dataset: {len(validation_dataset)} images\")\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860eb724",
   "metadata": {},
   "source": [
    "## ðŸ§  9. Chargement du ModÃ¨le PrÃ©-entraÃ®nÃ© **GoogLeNet**\n",
    "- Remplacement de la partie **Fully Connected (FC)** par un `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db743dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Nombre de classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Charger le modÃ¨le prÃ©-entraÃ®nÃ©\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Geler les couches convolutives\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Remplacer la couche fully connected\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Pas besoin de device, on reste sur CPU\n",
    "print(\"ModÃ¨le prÃªt sur CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef946b",
   "metadata": {},
   "source": [
    "## âš™ï¸ 10. Configuration de lâ€™EntraÃ®nement\n",
    "- Learning Rate\n",
    "- Loss Function\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb35a6",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ 11. Boucle dâ€™EntraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Stocker les valeurs\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfaa28",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“Š 12. Ã‰valuation & Test du ModÃ¨le\n",
    "- Accuracy / Loss\n",
    "- Matrice de confusion (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a79ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_preds.extend(predicted.tolist())\n",
    "\n",
    "# Calculer l'accuracy\n",
    "test_acc = sum([a==b for a,b in zip(all_labels, all_preds)]) / len(all_labels)\n",
    "print(f\"Accuracy sur le test set : {test_acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Matrice de confusion :\\n\", cm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "plt.xlabel(\"PrÃ©dictions\")\n",
    "plt.ylabel(\"VÃ©ritables classes\")\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs+1)\n",
    "\n",
    "# Loss\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Ã‰volution du Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Ã‰volution de l'Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e79b",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 13. Sauvegarde du ModÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../models/googlenet_complete.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e240e",
   "metadata": {},
   "source": [
    "## âœ… 14. Conclusion & Observations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
